{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AVAMET', 'CLEAN_DATA', 'GVA', 'IVIA']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtener el directorio actual\n",
    "path = os.getcwd()\n",
    "\n",
    "# Concatenar el directorio actual con la ruta al directorio de los datos\n",
    "path_data = os.path.join(path, \"raw\")\n",
    "os.listdir(path_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_save = os.path.join(path_data, \"CLEAN_DATA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_cols = ['Fecha', 'Hora', 'Velocidad', 'Direccion', 'Temperatura', 'Humedad']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cada carpeta hace referencia a las diferentes fuentes de los datos climáticos. Puesto que cada formato tiene sus peculiaridades, vamos a limpiar los datos para obtener nuestros datos de forma única."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CLEANING GVA DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_Alzira.txt', 'data_Artana.txt', 'data_Beniganim.txt', 'data_Cirat.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_gva = os.path.join(path_data, \"GVA\")\n",
    "os.listdir(path_gva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = ['FECHA', 'HORA', 'Veloc.', 'Direc.', 'Temp.', 'H.Rel.']\n",
    "\n",
    "fechas = {\n",
    "    'Alzira': ('2016-06-16', '2016-06-18'),\n",
    "    'Artana': ('2016-07-25', '2016-07-27'),\n",
    "    'Beniganim': ('2018-08-06', '2018-08-08'),\n",
    "    'Cirat': ('2015-06-07', '2015-06-10')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_Alzira.txt\n",
      "4009    00:00:00\n",
      "4010    01:00:00\n",
      "4011    02:00:00\n",
      "4012    03:00:00\n",
      "4013    04:00:00\n",
      "          ...   \n",
      "4076    19:00:00\n",
      "4077    20:00:00\n",
      "4078    21:00:00\n",
      "4079    22:00:00\n",
      "4080    23:00:00\n",
      "Name: HORA, Length: 72, dtype: object\n",
      "data_Artana.txt\n",
      "4945    00:00:00\n",
      "4946    01:00:00\n",
      "4947    02:00:00\n",
      "4948    03:00:00\n",
      "4949    04:00:00\n",
      "          ...   \n",
      "5012    19:00:00\n",
      "5013    20:00:00\n",
      "5014    21:00:00\n",
      "5015    22:00:00\n",
      "5016    23:00:00\n",
      "Name: HORA, Length: 72, dtype: object\n",
      "data_Beniganim.txt\n",
      "5098    00:00:00\n",
      "5099    01:00:00\n",
      "5100    02:00:00\n",
      "5101    03:00:00\n",
      "5102    04:00:00\n",
      "          ...   \n",
      "5165    19:00:00\n",
      "5166    20:00:00\n",
      "5167    21:00:00\n",
      "5168    22:00:00\n",
      "5169    23:00:00\n",
      "Name: HORA, Length: 72, dtype: object\n",
      "data_Cirat.txt\n",
      "3767    00:00:00\n",
      "3768    01:00:00\n",
      "3769    02:00:00\n",
      "3770    03:00:00\n",
      "3771    04:00:00\n",
      "          ...   \n",
      "3858    19:00:00\n",
      "3859    20:00:00\n",
      "3860    21:00:00\n",
      "3861    22:00:00\n",
      "3862    23:00:00\n",
      "Name: HORA, Length: 96, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for file, key in zip(os.listdir(path_gva), fechas.keys()):\n",
    "    print(file)\n",
    "    # Leemos el fichero y seleccionamos las variables de interés\n",
    "    df = pd.read_csv(os.path.join(path_gva, file), sep=\"\\t\", encoding='ISO-8859-1')\n",
    "    df = df[vars]\n",
    "    df['FECHA'] = pd.to_datetime(df['FECHA'], format='%d/%m/%Y')\n",
    "\n",
    "    # Fijamos el intervalo de interés\n",
    "    inicio, fin = fechas[key]\n",
    "    df = df[(df['FECHA'] >= inicio) & (df['FECHA'] <= fin)]\n",
    "    \n",
    "\n",
    "    # Cambiamos el formato de las fechas y variables.\n",
    "    df['HORA'] = df['HORA'].astype('int64').astype('str')\n",
    "    df['HORA'] = pd.to_datetime(df['HORA'], format='%H').dt.strftime('%H:%M:%S')\n",
    "    print(df.HORA)\n",
    "    try:\n",
    "        df['Veloc.'] = df['Veloc.'].astype('float')\n",
    "        df['Temp.'] = df['Temp.'].astype('float')\n",
    "    except:\n",
    "        df['Veloc.'] = df['Veloc.'].str.replace(\",\", \".\").astype('float')\n",
    "        df['Temp.'] = df['Temp.'].str.replace(\",\", \".\").astype('float')\n",
    "    df['Direc.'] = df['Direc.'].astype('float')\n",
    "    df['H.Rel.'] = df['H.Rel.'].astype('float') \n",
    "    df['Veloc.'] = df['Veloc.'] * 3.6 #  m/s -> km/h\n",
    "\n",
    "    # Cambiamos el nombre de las variables\n",
    "    df.columns = name_cols\n",
    "\n",
    "    # Guardamos el fichero\n",
    "    name = file.split(\".\")[0]\n",
    "    df.to_csv(os.path.join(path_save, name), sep = \";\", decimal = \".\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.path.join(path_save, \"data_Alzira\"), sep = \";\", decimal = \".\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cleaning AVAMET data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['data_Beneixama.csv', 'data_Beniarda.csv', 'data_VallGallinera.csv']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_avamet = os.path.join(path_data, \"AVAMET\")\n",
    "os.listdir(path_avamet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_cols = ['Fecha', 'Hora', 'Direccion', 'Velocidad', 'Temperatura', 'Humedad']\n",
    "vars = ['Fecha', 'Hora', 'vent gra_mit', 'vent vel_mit', \"temp mit_mit\", \"hrel mit_mit\"]\n",
    "fechas = {\n",
    "    'Beneixama': ('2019-07-15', '2019-07-15'),\n",
    "    'Beniarda': ('2020-08-28', '2020-08-29'),\n",
    "    'VallGallinera': ('2020-08-08', '2020-08-10')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_Beneixama.csv\n",
      "data_Beniarda.csv\n",
      "data_VallGallinera.csv\n"
     ]
    }
   ],
   "source": [
    "for file, key in zip(os.listdir(path_avamet), fechas.keys()):\n",
    "    print(file)\n",
    "    # Leemos el fichero. Seleccionamos y creamos las variables de interés\n",
    "    df = pd.read_csv(os.path.join(path_avamet, file))\n",
    "    df[['Fecha', 'Hora']] = df['data ini'].str.split(\" \", expand = True)\n",
    "    df['Fecha'] = pd.to_datetime(df['Fecha'], format='%m/%d/%y')\n",
    "    df['Hora'] = df['Hora'].str.zfill(5)\n",
    "    df['Hora'] = pd.to_datetime(df['Hora'], format='%H:%M').dt.time\n",
    "    df = df[vars]\n",
    "\n",
    "    # Fijamos el intervalo de interés\n",
    "    inicio, fin = fechas[key]\n",
    "    df = df[(df['Fecha'] >= inicio) & (df['Fecha'] <= fin)]\n",
    "\n",
    "    # Cambiamos el formato de las fechas y variables.\n",
    "    try:\n",
    "        df['vent vel_mit'] = df['vent vel_mit'].astype('float')\n",
    "        df['temp mit_mit'] = df['temp mit_mit'].astype('float')\n",
    "        df['hrel mit_mit'] = df['hrel mit_mit'].astype('float')\n",
    "    except:\n",
    "        df['vent vel_mit'] = df['vent vel_mit'].astype('str').str.replace(\",\", \".\").astype('float')\n",
    "        df['temp mit_mit'] = df['temp mit_mit'].astype('str').str.replace(\",\", \".\").astype('float')\n",
    "        df['hrel mit_mit'] = df['hrel mit_mit'].astype('str').str.replace(\",\", \".\").astype('float')\n",
    "    \n",
    "    # Cambiamos el nombre de las variables y guradamos el fichero.\n",
    "    df.columns = name_cols\n",
    "    name = file.split(\".\")[0]\n",
    "    df.to_csv(os.path.join(path_save, name), sep = \";\", decimal = \".\", index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Cleaning IVIA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data_Artana.txt'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_ivia = os.path.join(path_data, \"IVIA\")\n",
    "os.listdir(path_ivia)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars = ['fecha', 'hora', 'tmedia', 'hmedia', 'viento', 'direccion_num']\n",
    "name_cols = ['Fecha', 'Hora', 'Temperatura', 'Humedad', 'Velocidad', 'Direccion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_Artana.txt\n",
      "data_Bolulla.txt\n",
      "data_Carcaixent.txt\n",
      "data_Chella.txt\n",
      "data_Gatova.txt\n",
      "data_ValldEbo.txt\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(path_ivia):\n",
    "    print(file)\n",
    "\n",
    "    # Leemos el fichero y seleccionamos las variables de interés\n",
    "    with open(os.path.join(path_ivia, file), 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    df = pd.json_normalize(data, 'data')\n",
    "    df = df[vars]\n",
    "\n",
    "    # Cambiamos el formato de las fechas y nombre de las columnas.\n",
    "    df['fecha'] = pd.to_datetime(df['fecha'], format='%d/%m/%Y')\n",
    "\n",
    "    # Convertir la columna 'Hora' a formato datetime\n",
    "    df['hora'] = pd.to_datetime(df['hora'], format='%H:%M', errors='coerce')\n",
    "\n",
    "    # Aplicar una corrección a los valores de hora que son mayores o iguales a 24\n",
    "    df['hora'] = pd.to_datetime(df['hora'].mask(df['hora'].dt.hour >= 24, df['hora'].dt.strftime('00:%M')), format='%H:%M', errors='coerce').dt.strftime('%H:%M:%S')\n",
    "\n",
    "    # Guardamos el fichero\n",
    "    df.columns = name_cols\n",
    "    name = file.split(\".\")[0]\n",
    "    df.to_csv(os.path.join(path_save, name), sep = \";\", decimal = \".\", index = False)    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f8852db887e4898adf41e8dfeeeb2275e3a600f3c56ac76bec9205a61cab2c77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
